# GTC2017 Instructions

If you are lost in one of the steps, just checkout one of the `gtc2017-sol-step` branches to get the solution and start working for the next step.

## Step 0: Execute and Profile FWI
- Read the README.md to know how to compile & profile FWI

- Execute the application with the 'profile' dataset:
  `bin/fwi ../data/fwi_params.txt ../data/fwi_frequencies.profile.txt`

- Recompile with OpenMP enabled and execute the application again

- Recompile with Profiling enabled and profile FWI

## Step 1: Parallelize the application using OpenACC pragmas

- First implementation will make use of CUDA Unified Memory also called 'Managed Memory' thus, you will need to modify the `pgcc` flags in CMakeLists.txt file and add the `managed` to the `-ta=tesla` target

- Initialize the data in the gpu by puting a `#pragma acc kernels` in function `set_array_to_constant` of `src/fwi_kernels.c` file.

- Then put pragmas in all compute-intensive parts of src/fwi_propagator.c file.
  As you will have seen in step 0, those parts are:
   - vcell_TL
   - vcell_TR
   - vcell_BL
   - vcell_BR
   - scell_TL
   - scell_TR
   - scell_BL
   - scell_BR
  Although there are 8 parallel regions to complete, in reality all 'vcell' pragmas will be the same, and all 'scell' pragmas also

- Read all compiler output to find whether `pgcc` is generating a kernel or not (`-Minfo=accel` flag).

- If `pgcc` refuses to parallelize a compute region because thinks there is a dependence and you know there is none, you can put a `#pragma acc loop independent` to force the compiler to parallelize the region

- Functions that are called inside OpenACC parallel regions must be either inlined or declared with the `#pragma acc routine <type>` specifier.
  In our case you will have to add the `#pragma acc routine seq` to all `device` functions in `include/fwi/fwi_propagator.h` file

- In src/fwi_kernel.c you will find the `propagate_shot` function that advances all timesteps.
  To prevent incorrect executions, put a barrier `#pragma acc wait` between iterations.

## Step 2: Profile & Optimize the applicaiton

- Profile the application with `nvprof`:
  `nvprof bin/fwi ../data/fwi_params.txt ../data/fwi_frequencies.profile.txt`

- Or use Nvidia Visual Profiler (if available)

- You will see that Occupancy is low ~18% (ideal should be between 50~100%)
  You will also see that the large number of registers is the main cause of this low occupancy.
  To increase occupancy in exchange of more memory pressure (with register spills into local memory) we limit the number of registers per kernel.
  To do this you have to modify the `pgcc` flags and add a `maxregcount:128` to the `-ta=tesla` flag.

- Execute and compare with the previous execution

## Step 3: OpenACC Data regions

- We will stop using CUDA Unified Memory by removing the `managed` flag of `pgcc` compiler in CMakeLists.txt

- Next, in every parallel region we will have to specify all movement of data with `copyin`, `copyout` or `copy` clauses
  For instance all vcell kernels:
```
const integer start  = ((nzf-nz0) + 2*HALO) * ((nxf-nx0) + 2*HALO) * (ny0 - HALO);
const integer end    = ((nzf-nz0) + 2*HALO) * ((nxf-nx0) + 2*HALO) * (nyf + HALO);
const integer nelems = end - start;

#pragma acc kernels copyin(szptr[start:nelems], sxptr[start:nelems], syptr[start:nelems], rho[start:nelems]) \
                    copy(vptr[start:nelems])
```
  Edit, all `vcell`, `scell` and `set_array_to_constant` functions.

- Execute & profile with nvprof

  Unfortunately H2D & D2H copies take more time that they should. In next step we will optimize this behaviour

## Step 4: Optimize data locality

- Use OpenACC `#pragma acc enter data create` and `#pragma acc exit data delete` pragmas to increase the locality in the GPU.
- In `alloc_memory_shot` function (`src/fwi_kernel.c`), and after all `malloc`s, put:

```
const integer datalen = numberOfCells;
coeff_t cc = *c;

#pragma acc enter data create(cc)
#pragma acc enter data create(cc.c11[:datalen])
#pragma acc enter data create(cc.c12[:datalen])
...
```
- In `free_memory_shot` function, before all calls to `free` put:
```
#pragma acc wait

#pragma acc exit data delete(c->c11)
#pragma acc exit data delete(c->c12)
...
#pragma acc exit data delete(c)
...
```
## Step 5: Add streams

- Add `async` clauses to use multiple streams and `#pragma acc wait(<stream list>)` to syncronize.


## Step 6: Add glue code to call CUDA Kernels

- Add preprocessor guards to compile the OpenACC code or to call the CUDA calls selectively:

In vcell/scell functions:
```
{
#if !defined(USE_CUDA)
    ...
    for
        for
           for
             ...
#else
    void* stream = acc_get_cuda_stream(phase)

    #pragma acc host_data use_device(szptr, sxptr, syptr, rho, vptr)
    {
        compute_component_vcell_TL_cuda(..., stream);
    }
#endif
};
```

