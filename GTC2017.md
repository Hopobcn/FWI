# GTC2017 Lab Instructions

In this lab you will make modifications to a real world mini application, then compile and execute the newly enhanced code in each step. Along the way, solutions are provided, so you can check your work, or take a peek if you get lost.

After login into the machine you should see 7 folders. `FWI` is the folder were this lab will take place and the other `FWI-sol-step` folders contain the solution for each step. Each one of those folders is just a checkout of the same FWI repository at a different branch. In case you want to discover all possible branches just type `git branch -a` inside one of those folders.

```bash
~# ls -l
total 28
drwxr-xr-x 10 root root 4096 Apr 27 11:26 FWI
drwxr-xr-x 10 root root 4096 Apr 27 11:26 FWI-sol-step1
drwxr-xr-x 10 root root 4096 Apr 27 11:27 FWI-sol-step2
drwxr-xr-x 10 root root 4096 Apr 27 11:27 FWI-sol-step3
drwxr-xr-x 10 root root 4096 Apr 27 11:27 FWI-sol-step4
drwxr-xr-x 10 root root 4096 Apr 27 11:27 FWI-sol-step5
drwxr-xr-x 10 root root 4096 Apr 27 11:27 FWI-sol-step6
```

Now enter the `FWI` folder:

```bash
$ cd FWI
```

Initially the `FWI` folder will be in `gtc2017` branch. If for some reason, during the labs, you want to check the difference between the solutions and your implementation you just have to execute: `git diff gtc2017-sol-step?` and you will see the differences between your current branch and the selected one. Or if you want to skip to the solution feel free to checkout the next branch.


## Step 0: Characterize FWI application

The initial step of every paralelization effort is to check whether it makes sense to apply a given programming model or not. In the case of accelerator programming a certain number of questions have to be answered before the first line of code is written. This includes:
* Understanding the program structure and how data is passed through the call tree
* Profiling the CPU-only version of the application and identifying computationally-intense "hot spots"
* Identify which loop nests dominate the runtime
* Are the loop nests suitable for an accelerator?
* Insuring that the algorithms you are considering for acceleration are safely parallel

> Although in the Step 0 of this lab you only have to profile & execute the sequential implementation and execute the OpenMP version, it can indeed take some time to execute. Feel free to just **read** the Step 0 and skip to Step 1 whenever you feel ready.

Firstly we will compile & execute the sequential version of FWI to annotate the initial performance:
```
$ mkdir build
$ cd build
$ cmake -DCMAKE_C_COMPILER=gcc ..
$ make
```
Then you execute FWI either using `make irun` or `bin/fwi ../data/fwi_params.txt ../data/fwi_frequencies.profile.txt`. You should see something like this:
```
$ make irun
[ 62%] Built target fwi-core
[ 87%] Built target fwi
[100%] outputs will be in /root/FWI/scripts/output/
PROJECT_SOURCE_DIR: /root/FWI
PROJECT_BINARY_DIR: /root/FWI/build/bin
COMPILER_ID:        GNU
---
/root/FWI/build/bin/fwi /root/FWI/data/fwi_params.txt /root/FWI/data/fwi_frequencies.profile.txt
---
Freq: 2.0 ------------------------
Processing 0-th gradient iteration.
        Gradient loop processed for the 0-th shot
FWI Program finished in 247.570841 seconds
[100%] Built target irun
```

Then we are going to profile FWI to search for hot spots using `nvprof --cpu-profiling on` (you can use `make iprofile` or call `nvprof` directly from console: `nvprof --cpu-profiling on bin/fwi ../data/fwi_params.txt ../data/fwi_frequencies.profile.txt`). Like the following example:

```bash
$ cmake -DCMAKE_C_COMPILER=gcc -DPROFILE=ON ..
$ make
$ nvprof --cpu-profiling on bin/fwi ../data/fwi_params.txt ../data/fwi_frequencies.profile.txt
Freq: 2.0 ------------------------
Processing 0-th gradient iteration.
        Gradient loop processed for the 0-th shot
FWI Program finished in 269.667140 seconds

======== CPU profiling result (bottom up):
Time(%)      Time  Name
 23.19%  62.5323s  velocity_propagator
 21.47%  57.9022s  compute_component_scell_BL
 21.34%  57.5522s  compute_component_scell_TR
 20.35%  54.8721s  compute_component_scell_BR
 11.13%  30.0111s  compute_component_scell_TL
  2.37%  6.38024s  compute_component_vcell_BR
  0.06%  150.01ms  __c_mzero4
  0.06%  150.01ms  | ???
  0.06%  150.01ms  __c_mset4
  0.03%  90.003ms  | ???
  0.02%  50.002ms  _IO_file_close
  0.02%  50.002ms  | _IO_file_close_it
  0.02%  50.002ms  |   _IO_fclose
  0.02%  50.002ms  |     safe_fclose
  0.01%  30.001ms  open
  0.01%  30.001ms  | _IO_file_open
  0.01%  30.001ms  |   _IO_file_fopen
  0.01%  30.001ms  |     ???
  0.01%  30.001ms  |       safe_fopen
  0.01%  30.001ms  munmap
  0.01%  30.001ms    __free

======== Data collected at 100Hz frequency
```
We can clearly see that the 99,85% of the execution time is split between *scell* functions (74,20%) and *vcell* functions (25,56%). If we take a look at those functions in **src/fwi_propagator.c** we will arrive to this conclusions:
1. They are embarrassingly parallel
2. We only have to think about how to parallelize one of the *scell* and *vcell* functions and then apply the same strategy to the rest.



Now we will recompile enabling OpenMP and execute the application to measure the performance of OpenMP vs the serial implementation:
```bash
$ cmake -DCMAKE_C_COMPILER=gcc -DUSE_OPENMP=ON ..
$ make irun
[ 62%] Built target fwi-core
[ 87%] Built target fwi
[100%] outputs will be in /root/FWI/scripts/output/
PROJECT_SOURCE_DIR: /root/FWI
PROJECT_BINARY_DIR: /root/FWI/build/bin
COMPILER_ID:        GNU
---
/root/FWI/build/bin/fwi /root/FWI/data/fwi_params.txt /root/FWI/data/fwi_frequencies.profile.txt
---
Freq: 2.0 ------------------------
Processing 0-th gradient iteration.
        Gradient loop processed for the 0-th shot
FWI Program finished in 54.091911 seconds
```
> Machine used: Intel i7-5930k (6-core with HT) + 1x Titan X (Pascal)

We got 4.61x compared the sequential version.

## Step 1: Adding OpenACC directives

In this step, you will add compute regions around your expensive parallel loop(s). We will assume that you already know some basics on OpenACC. In this lab we have used the **kernels** directive extensively but the **loop** directive could also been used.

Then, we will proceed to add `#pragma acc kernels` directives to all compute-intensive parts of our code. As we have seen in Step 0 those functions are:
   - vcell_TL
   - vcell_TR
   - vcell_BL
   - vcell_BR
   - scell_TL
   - scell_TR
   - scell_BL
   - scell_BR

For instance with *vcell_TL* the code will look like this:
```c
#pragma acc kernels
for(integer y=ny0; y < nyf; y++)
{
    for(integer x=nx0; x < nxf; x++)
    {
        for(integer z=nz0; z < nzf; z++)
        {
            const real lrho = rho_TL(rho, z, x, y, dimmz, dimmx);

            const real stx  = stencil_X( _SX, sxptr, dxi, z, x, y, dimmz, dimmx);
            const real sty  = stencil_Y( _SY, syptr, dyi, z, x, y, dimmz, dimmx);
            const real stz  = stencil_Z( _SZ, szptr, dzi, z, x, y, dimmz, dimmx);

            vptr[IDX(z,x,y,dimmz,dimmx)] += (stx  + sty  + stz) * dt * lrho;
        }
    }
}
```

In the first implementation we are going to use CUDA Unified Memory, also called 'CUDA Managed Memory'. You will need to modify the `pgcc` flags in **CMakeLists.txt** file and add the `managed` to the `-ta=tesla` target. It should look like this:
```cmake
set(OpenACC_C_FLAGS "${OpenACC_C_FLAGS} -ta=tesla,cuda8.0,cc20,cc35,cc50,cc60,lineinfo,managed")
```

After that we can try to compile the application (*notice that we changed* from **gcc** to **pgcc** ! and also we enabled OpenACC with **-DUSE_OPENACC=ON** !):
```bash
$ cmake -DCMAKE_C_COMPILER=pgcc -DUSE_OPENMP=OFF -DUSE_OPENACC=ON ..
$ make
[ 44%] Building C object src/CMakeFiles/fwi-core.dir/fwi_propagator.c.o
compute_component_vcell_TR:
    211, Generating implicit copy(vptr[:])
    212, Loop carried dependence due to exposed use of vptr[:*] prevents parallelization
         Accelerator kernel generated
        212, #pragma acc loop seq
        214, #pragma acc loop seq
        216, #pragma acc loop seq
    212, Accelerator scalar kernel generated
    214, Loop carried dependence due to exposed use of vptr[:*] prevents parallelization
         216, Loop carried dependence due to exposed use of vptr[:*] prevents parallelization
              Complex loop carried dependence of vptr-> prevents parallelization
```
But oops! the compiler detects a dependence and prevents a parallelization (it generates a scalar kernel!). Since we know that *vcell_TR* is embarrasingly parallel and there isn't a dependence we will put a `#pragma acc loop independent` before each iteration level to force the compiler to parallelize the region. Example:

```c
#pragma acc kernels
#pragma acc loop independent
for (integer y=ny0; y < nyf; y++) {
    #pragma acc loop independent
    for (integer x=nx0; x < nxf; x++) {
        #pragma acc loop independent
        for (integer z=nz0; z < nzf; z++) {
```
Then we can compile again:
```bash
$ make
Scanning dependencies of target fwi-core
[ 11%] Building C object src/CMakeFiles/fwi-core.dir/fwi_propagator.c.o
compute_component_vcell_TR:
    211, Generating implicit copy(vptr[:])
    213, Loop is parallelizable
         Generating Multicore code
        213, #pragma acc loop gang
    216, Loop is parallelizable
         219, Loop is parallelizable
              Accelerator kernel generated
              Generating Tesla code
             213, #pragma acc loop gang /* blockIdx.y */
             216, #pragma acc loop gang, vector(4) /* blockIdx.z threadIdx.y */
             219, #pragma acc loop gang, vector(32) /* blockIdx.x threadIdx.x */
[ 22%] Linking C static library ../lib/libfwi-core.a
[ 55%] Built target fwi-core
[ 66%] Linking C executable ../../bin/fwi-data-generator
[ 77%] Built target fwi-data-generator
[ 88%] Linking C executable ../bin/fwi
[100%] Built target fwi
```
Functions that are called inside OpenACC parallel regions must be either inlined or declared with the `#pragma acc routine <type>` specifier. Since we are compiling with `-O3` `pgcc` decided to inline *rho_TL*, *stencil_X*, *stencil_Y* & *stencil_Z*. But what happens if we compile with `-O0` ?

```
$ cmake -DCMAKE_C_COMPILER=pgcc -DCMAKE_BUILD_TYPE=Debug -DUSE_OPENMP=OFF -DUSE_OPENACC=ON ..
$ make
compute_component_vcell_TR:
    211, Accelerator region ignored
    221, Accelerator restriction: call to 'rho_TR' with no acc routine information
```
So you will have to add the `#pragma acc routine seq` directive to every function (`include/fwi/fwi_propagator.h` file) that is called inside a OpenACC kernels region. This way the compiler will know it is a device-callable function (`-O0`) or it will just inline the whole function (`-O3`).

Then *proceed with the rest of the vcell & scell functions*, remember that the strategy is the same as the example shown here.


As an extra, you move the data initialization to the gpu by puting a `#pragma acc kernels` before the loop in function `set_array_to_constant` of `src/fwi_kernels.c`. Example:

```c
void set_array_to_constant(...)
{
    #pragma acc kernels
    for ( integer i = 0; i < length; i++ )
        array[i] = value
}
```

Finally, to prevent incorrect executions, we have to put a barrier (`#pragma acc wait`) between iterations of the time propagation loop (`propagate_shot` function of `src/fwi_kernel.c` file)

#### Benchmarking

After implementing all *scell* and *vcell* functions we can proceed to measure the execution time:
```
$ make irun
Scanning dependencies of target irun
[100%] outputs will be in /root/FWI-sol-step1/scripts/output/
PROJECT_SOURCE_DIR: /root/FWI-sol-step1
PROJECT_BINARY_DIR: /root/FWI-sol-step1/build/bin
COMPILER_ID:        PGI
---
/root/FWI-sol-step1/build/bin/fwi /root/FWI-sol-step1/data/fwi_params.txt /root/FWI-sol-step1/data/fwi_frequencies.profile.txt
---
MPI rank 0 with GPU 0 (4)
Freq: 2.0 ------------------------
Processing 0-th gradient iteration.
        Gradient loop processed for the 0-th shot
FWI Program finished in 3.582803 seconds
[100%] Built target irun
```
That is, 3.58s or 15,1x faster than the original OpenMP version.

## Step 2: Profile & Optimize the applicaiton

In Step 2 we have to profile the application to find possible weaknesses and optimization opportunities. We could use *Nvidia Visual Profiler* for a graphical assestment of the application or `pgprof`/`nvprof` for a command-line visualization. For simplicity in this lab we are going to use `nvprof`:
```bash
$ nvprof bin/fwi ../data/fwi_params.txt ../data/fwi_frequencies.profile.txt 
==2015== NVPROF is profiling process 2015, command: bin/fwi ../data/fwi_params.txt ../data/fwi_frequencies.profile.txt
MPI rank 0 with GPU 0 (4)
Freq: 2.0 ------------------------
Processing 0-th gradient iteration.
        Gradient loop processed for the 0-th shot
FWI Program finished in 3.608973 seconds
==2015== Profiling application: bin/fwi ../data/fwi_params.txt ../data/fwi_frequencies.profile.txt
==2015== Profiling result:
Time(%)      Time     Calls       Avg       Min       Max  Name
 21.24%  630.88ms       300  2.1029ms  179.75us  6.0901ms  compute_component_scell_TR_648_gpu
 21.18%  629.15ms       300  2.0972ms  163.27us  6.0794ms  compute_component_scell_BR_901_gpu
 20.05%  595.66ms       300  1.9855ms  170.41us  5.7658ms  compute_component_scell_BL_1028_gpu
 13.11%  389.54ms       300  1.2985ms  113.93us  3.7244ms  compute_component_scell_TL_774_gpu
  8.65%  256.91ms        58  4.4295ms  3.4027ms  7.6222ms  set_array_to_constant_52_gpu
  4.34%  128.80ms       900  143.11us  14.881us  406.93us  compute_component_vcell_BR_291_gpu
  3.82%  113.39ms       900  125.99us  12.320us  356.43us  compute_component_vcell_TL_183_gpu
  3.81%  113.27ms       900  125.86us  12.737us  362.80us  compute_component_vcell_BL_345_gpu
  3.80%  112.95ms       900  125.50us  12.960us  357.74us  compute_component_vcell_TR_237_gpu

==2015== Unified Memory profiling result:
Device "TITAN X (Pascal) (0)"
   Count  Avg Size  Min Size  Max Size  Total Size  Total Time  Name
    3810         -         -         -           -  123.7249ms  GPU Page fault groups
Total CPU Page faults: 1
...
```
We can see that *scell* kernels take more time than *vcell* kernels.

Now we will obtain the occupancy for again with `nvprof`:
```bash
$ nvprof --metrics achieved_occupancy bin/fwi ../data/fwi_params.txt ../data/fwi_frequencies.profile.txt                                         
==2042== NVPROF is profiling process 2042, command: bin/fwi ../data/fwi_params.txt ../data/fwi_frequencies.profile.txt
MPI rank 0 with GPU 0 (4)
Freq: 2.0 ------------------------
Processing 0-th gradient iteration.
        Gradient loop processed for the 0-th shot
FWI Program finished in 74.528802 seconds
==2042== Profiling application: bin/fwi ../data/fwi_params.txt ../data/fwi_frequencies.profile.txt
==2042== Profiling result:
==2042== Metric result:
Invocations                               Metric Name                        Metric Description         Min         Max         Avg
Device "TITAN X (Pascal) (0)"
    Kernel: compute_component_vcell_TR_237_gpu
        900                        achieved_occupancy                        Achieved Occupancy    0.373496    0.395784    0.384743
    Kernel: compute_component_vcell_BR_291_gpu
        900                        achieved_occupancy                        Achieved Occupancy    0.328186    0.339323    0.334482
    Kernel: compute_component_scell_TR_648_gpu
        300                        achieved_occupancy                        Achieved Occupancy    0.181673    0.184740    0.184103
    Kernel: compute_component_vcell_TL_183_gpu
        900                        achieved_occupancy                        Achieved Occupancy    0.372697    0.395206    0.384698
    Kernel: set_array_to_constant_52_gpu
         58                        achieved_occupancy                        Achieved Occupancy    0.924032    0.955698    0.939211
    Kernel: compute_component_scell_BR_901_gpu
        300                        achieved_occupancy                        Achieved Occupancy    0.181021    0.184291    0.183419
    Kernel: compute_component_scell_TL_774_gpu
        300                        achieved_occupancy                        Achieved Occupancy    0.180216    0.182999    0.182345
    Kernel: compute_component_scell_BL_1028_gpu
        300                        achieved_occupancy                        Achieved Occupancy    0.180706    0.184159    0.183399
    Kernel: compute_component_vcell_BL_345_gpu
        900                        achieved_occupancy                        Achieved Occupancy    0.372508    0.394239    0.384888
```
Unfortunately the achieved occupancy of *scell* kernels is too low. We are going to focus in increasing that occupancy. One way to do this is limiting the number of registers per thread block to achieve more occupancy in exchange of more memory pressure (register spills). Then we can add  to`pgcc` the flag `-ta=tesla,maxregcount:128` (`CMakeLists.txt` file).

#### Benchmarking

Now we can execute FWI and compare with the previous step

```bash
# make irun
[ 62%] Built target fwi-core
[ 87%] Built target fwi
[100%] outputs will be in /root/FWI-sol-step2/scripts/output/
PROJECT_SOURCE_DIR: /root/FWI-sol-step2
PROJECT_BINARY_DIR: /root/FWI-sol-step2/build/bin
COMPILER_ID:        PGI
---
/root/FWI-sol-step2/build/bin/fwi /root/FWI-sol-step2/data/fwi_params.txt /root/FWI-sol-step2/data/fwi_frequencies.profile.txt
---
MPI rank 0 with GPU 0 (4)
Freq: 2.0 ------------------------
Processing 0-th gradient iteration.
        Gradient loop processed for the 0-th shot
FWI Program finished in 3.480004 seconds
[100%] Built target irun
```
We improved a little bit from 15,1x to 15,54x.

## Step 3: Moving from Unified Memory to OpenACC Data clauses

Unified Memory can be very inneficient in older GPU generations and an experienced programmer with detailed knowledge of the application algorithm can outperform the Unified Memory.

Therefore in this step we are going to focus on migrating from the CUDA Unified Memory to a more manual movement of data using OpenACC directives.

Firstly we have the `managed` flag of `pgcc` compiler in `CMakeLists.txt` and then, in every parallel region we will have to specify all data dependences with `copyin`, `copyout` or `copy` clauses.

For instance all vcell kernels:
```c
const integer start  = ((nzf-nz0) + 2*HALO) * ((nxf-nx0) + 2*HALO) * (ny0 - HALO);
const integer end    = ((nzf-nz0) + 2*HALO) * ((nxf-nx0) + 2*HALO) * (nyf + HALO);
const integer nelems = end - start;

#pragma acc kernels copyin(szptr[start:nelems], sxptr[start:nelems], syptr[start:nelems], rho[start:nelems]) \
                    copy(vptr[start:nelems])
```

Edit, all `vcell`, `scell` and `set_array_to_constant` functions. 

Arrays that are declared as `const real* restrict` can be considered as input dependences (`copyin`) and arrays that are just `real* restrict` are input/output dependences (`copy`)

#### Benchmarking

Now we can run the application again:
```bash
# make irun
[ 62%] Built target fwi-core
[ 87%] Built target fwi
Scanning dependencies of target irun
[100%] outputs will be in /root/FWI-sol-step2/scripts/output/
PROJECT_SOURCE_DIR: /root/FWI-sol-step2
PROJECT_BINARY_DIR: /root/FWI-sol-step2/build/bin
COMPILER_ID:        PGI
---
/root/FWI-sol-step2/build/bin/fwi /root/FWI-sol-step2/data/fwi_params.txt /root/FWI-sol-step2/data/fwi_frequencies.profile.txt
---
MPI rank 0 with GPU 0 (4)
Freq: 2.0 ------------------------
Processing 0-th gradient iteration.
        Gradient loop processed for the 0-th shot
FWI Program finished in 40.955888 seconds
[100%] Built target irun
```
Since we got an slowdonw we should profile to get an insight of what is happening:

```
# nvprof bin/fwi ../data/fwi_params.txt ../data/fwi_frequencies.profile.txt                                                                      
==10572== NVPROF is profiling process 10572, command: bin/fwi ../data/fwi_params.txt ../data/fwi_frequencies.profile.txt
MPI rank 0 with GPU 0 (4)
Freq: 2.0 ------------------------
Processing 0-th gradient iteration.
        Gradient loop processed for the 0-th shot
FWI Program finished in 43.766681 seconds
==10572== Profiling application: bin/fwi ../data/fwi_params.txt ../data/fwi_frequencies.profile.txt
==10572== Profiling result:
Time(%)      Time     Calls       Avg       Min       Max  Name
 78.21%  24.8734s     61258  406.04us  85.955us  1.0648ms  [CUDA memcpy HtoD]
 12.99%  4.13221s     10800  382.61us  80.516us  1.0165ms  [CUDA memcpy DtoH]
  2.07%  658.96ms       300  2.1965ms  190.79us  6.3724ms  compute_component_scell_TR_681_gpu
  2.07%  657.51ms       300  2.1917ms  178.76us  6.6763ms  compute_component_scell_BR_960_gpu
  1.93%  612.74ms       300  2.0425ms  183.02us  6.2133ms  compute_component_scell_BL_1100_gpu
  1.26%  400.02ms       300  1.3334ms  116.39us  3.8245ms  compute_component_scell_TL_820_gpu
  0.41%  129.30ms       900  143.67us  12.736us  441.72us  compute_component_vcell_BR_306_gpu
  0.36%  113.23ms       900  125.81us  10.817us  377.71us  compute_component_vcell_TR_247_gpu
  0.35%  112.77ms       900  125.30us  11.105us  382.90us  compute_component_vcell_TL_188_gpu
  0.35%  112.22ms       900  124.69us  10.688us  379.60us  compute_component_vcell_BL_365_gpu
  0.01%  2.6016ms        58  44.855us  44.737us  45.442us  set_array_to_constant_52_gpu
```
And unfortunately we see that H2D & D2H copies take 91.2% of the GPU execution time. In following steps we will optimize this behaviour.

## Step 4: Optimize data locality

In this step we are going to use OpenACC Data regions to increase the locality of data in the GPU. OpenACC offers two 
sets of directives to to that task:
* `#pragma acc data` clause can be used to delcare a `scope` where the data resides in the GPU.
* `#pragma enter data` / `#pragma exit data` clauses can be used when de declaration and destruction of data resides in different scopes.

In our case, we are going to use the `#pragma acc enter data create` and `#pragma acc exit data delete` pragmas to increase the locality in the GPU. In `alloc_memory_shot` function (`src/fwi_kernel.c`), and after all `malloc`s, put:

```c
const integer datalen = numberOfCells;
coeff_t cc = *c;

#pragma acc enter data create(cc)
#pragma acc enter data create(cc.c11[:datalen])
#pragma acc enter data create(cc.c12[:datalen])
...
```

- In `free_memory_shot` function, before all calls to `free` put:

```c
#pragma acc wait

#pragma acc exit data delete(c->c11)
#pragma acc exit data delete(c->c12)
...
#pragma acc exit data delete(c)
...
```

#### Benchmarking

Now we can run the application again:

```
# make irun
[ 62%] Built target fwi-core
[ 87%] Built target fwi
[100%] outputs will be in /root/FWI-sol-step2/scripts/output/
PROJECT_SOURCE_DIR: /root/FWI-sol-step2
PROJECT_BINARY_DIR: /root/FWI-sol-step2/build/bin
COMPILER_ID:        PGI
---
/root/FWI-sol-step2/build/bin/fwi /root/FWI-sol-step2/data/fwi_params.txt /root/FWI-sol-step2/data/fwi_frequencies.profile.txt
---
MPI rank 0 with GPU 0 (4)
Freq: 2.0 ------------------------
Processing 0-th gradient iteration.
        Gradient loop processed for the 0-th shot
FWI Program finished in 3.294508 seconds
[100%] Built target irun
```
And finally solved the problems we had in step 3, and increased the performance respecting step 1 & 2.

## Step 5: Asynchronous execution

OpenACC follows the same semantics as CUDA regarding streams. By default, kernels and memory copies are executed in the default stream which imposes seralization between kerenls and memory transfers.

This step will focus on using multiple streams to perform H2D,D2H copies concurrently with Kernel executions. 
Also another important aspect of this step is that allows us to optimize HALO copies between multiple-GPUs when MPI is enabled. This is achieved by calculating first the HALO regions and then, while we exchange the HALO regions with MPI, we execute the rest of the volume.

To differenciate the different parts of the application we are going to use the `phase_t` enum present in `include/fwi/fwi_propagator.h::78`:
```c
typedef enum {ONE_R, ONE_L, TWO, H2D, D2H} phase_t;
```
Then we can add the `async` clause to use multiple streams in *scell* and *vcell* functions and `#pragma acc wait(<stream list>)` to syncronize those streams.
As an example:
```c
#pragma acc kernels ... async(phase) wait(H2D)
```
  or
```c
#pragma acc wait(TWO, D2H)
```

#### Benchmarking

```bash
# make irun
[ 62%] Built target fwi-core
[ 87%] Built target fwi
[100%] outputs will be in /root/FWI-sol-step2/scripts/output/
PROJECT_SOURCE_DIR: /root/FWI-sol-step2
PROJECT_BINARY_DIR: /root/FWI-sol-step2/build/bin
COMPILER_ID:        PGI
---
/root/FWI-sol-step2/build/bin/fwi /root/FWI-sol-step2/data/fwi_params.txt /root/FWI-sol-step2/data/fwi_frequencies.profile.txt
---
MPI rank 0 with GPU 0 (4)
Freq: 2.0 ------------------------
Processing 0-th gradient iteration.
        Gradient loop processed for the 0-th shot
FWI Program finished in 3.269044 seconds
[100%] Built target irun
```
Again we obtained a slight improvement.

## Step 6: Add glue code to call CUDA Kernels

If we are not satisfied with the OpenACC kernel performance or we want to use a functionality only present in CUDA we interoperate with CUDA.

In CUDA it is essential maximize memory bandwidth to achieve maximum performance. In our case we could take advantage of shared memory and shuffle instructions to boost the performance. 

Due time limitations we already provide those optimized kernels (`src/fwi_propagator.cu`) and your job is to add the boilerplate code to use those kernels from OpenACC.

We also provide the necessary modifications in `CMakeLists.txt` for compiling with `nvcc` and linking with `pgcc`.

In summary. You will have to add `#pragma acc host_data use_device` directives to pass the *device pointers* allocated by OpenACC to our CUDA kernels and call `acc_get_cuda_stream` to forward the current stream to cuda in every **vcell**/**scell** function. Example:
```c
{
#if !defined(USE_CUDA)
    <... previous OpenACC impl. ...>
#else
    void* stream = acc_get_cuda_stream(phase)

    #pragma acc host_data use_device(szptr, sxptr, syptr, rho, vptr)
    {
        compute_component_vcell_TL_cuda(..., stream);
    }
#endif
};
```

#### Benchmarking

Then recompile with USE_CUDA_KERNELS=ON and execute FWI:

```bash
$ cmake -DCMAKE_C_COMPILER=pgcc -DUSE_OPENMP=OFF -DUSE_OPENACC=ON -DUSE_CUDA_KERNELS=ON ..
$ make irun
[ 27%] Built target fwi-core-cuda
[ 72%] Built target fwi-core
[ 90%] Built target fwi
[100%] outputs will be in /root/FWI-sol-step6/scripts/output/
PROJECT_SOURCE_DIR: /root/FWI-sol-step6
PROJECT_BINARY_DIR: /root/FWI-sol-step6/build/bin
COMPILER_ID:        PGI
---
/root/FWI-sol-step6/build/bin/fwi /root/FWI-sol-step6/data/fwi_params.txt /root/FWI-sol-step6/data/fwi_frequencies.profile.txt
---
MPI rank 0 with GPU 0 (4)
Freq: 2.0 ------------------------
Processing 0-th gradient iteration.
        Gradient loop processed for the 0-th shot
FWI Program finished in 2.798860 seconds
[100%] Built target irun
```
This time we improved the overall speedup to 19.3x
